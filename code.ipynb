{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6aa3f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    PythonCodeTextSplitter,\n",
    "    \n",
    ")\n",
    "from typing import List\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# --- HuggingFace embedding model setup ---\n",
    "# Load MiniLM v2 for embeddings once\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "\n",
    "# --- Language to code splitter mapping ---\n",
    "code_splitter_map = {\n",
    "    'python': PythonCodeTextSplitter,\n",
    "    \n",
    "}\n",
    "\n",
    "def split_text_by_language(text: str, language: str, chunk_size=1000, chunk_overlap=200) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text using language-specific code splitters or generic RecursiveCharacterTextSplitter for natural language.\n",
    "    \"\"\"\n",
    "    language = language.lower()\n",
    "\n",
    "    if language in code_splitter_map:\n",
    "        splitter_cls = code_splitter_map[language]\n",
    "        splitter = splitter_cls(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    else:\n",
    "        # Use generic splitter for natural languages or unknown code languages\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "        )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "\n",
    "def create_sections_code(\n",
    "    category_id, blob_name, page_map, mode, language,\n",
    "    blob_Connection_String, blob_container_name,\n",
    "    base_threshold, buffer_percent, overlap_sent_count\n",
    "):\n",
    "    chunk_id_prefix = blob_name.replace(\" \", \"_\").replace(\".\", \"_\")\n",
    "    input_data = []\n",
    "\n",
    "    try:\n",
    "        # Combine all page texts\n",
    "        all_text = \"\"\n",
    "        page_positions = []  # (start_pos, end_pos, page_num)\n",
    "        current_pos = 0\n",
    "        for page_num, _, text in page_map:\n",
    "            cleaned_text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            if cleaned_text:\n",
    "                start_pos = current_pos\n",
    "                all_text += cleaned_text + \"\\n\\n\"\n",
    "                current_pos = len(all_text)\n",
    "                page_positions.append((start_pos, current_pos, page_num))\n",
    "\n",
    "        chunk_overlap = int(base_threshold * buffer_percent / 100)\n",
    "\n",
    "        # Split text by language\n",
    "        chunks = split_text_by_language(all_text, language, base_threshold, chunk_overlap)\n",
    "\n",
    "        def split_into_sentences(text):\n",
    "            import re\n",
    "            sentence_endings = re.compile(r'(?<=[.!?]) +')\n",
    "            return sentence_endings.split(text)\n",
    "\n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            chunk_start = all_text.find(chunk)\n",
    "            chunk_end = chunk_start + len(chunk)\n",
    "\n",
    "            chunk_pages = set()\n",
    "            for start_pos, end_pos, page_num in page_positions:\n",
    "                if (chunk_start <= end_pos and chunk_end >= start_pos):\n",
    "                    chunk_pages.add(page_num)\n",
    "\n",
    "            if chunk_pages:\n",
    "                start_page = min(chunk_pages)\n",
    "                end_page = max(chunk_pages)\n",
    "                page_range = f\"{start_page}-{end_page}\" if start_page != end_page else str(start_page)\n",
    "\n",
    "                # Add overlap sentences from previous chunk if needed\n",
    "                if overlap_sent_count > 0 and chunk_idx > 0:\n",
    "                    prev_chunk = chunks[chunk_idx - 1]\n",
    "                    prev_sentences = split_into_sentences(prev_chunk)\n",
    "                    overlap_text = ' '.join(prev_sentences[-overlap_sent_count:])\n",
    "                    if overlap_text and not chunk.startswith(overlap_text):\n",
    "                        chunk = overlap_text + \" \" + chunk\n",
    "\n",
    "                cleaned_chunk = re.sub(r'\\n{3,}', '\\n\\n', chunk.strip())\n",
    "\n",
    "                # Get embedding using HF model\n",
    "                #embedding = get_hf_embedding(cleaned_chunk)\n",
    "\n",
    "                input_data.append(cleaned_chunk)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file '{blob_name}': {e}\")\n",
    "        return []\n",
    "\n",
    "    return input_data\n",
    "\n",
    "# --------- Placeholder for Groq LLM call ---------\n",
    "# You can implement a separate function to call Groq API when you need LLM functionality,\n",
    "# For example:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99da57e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_id = \"FormRechonizer\" \n",
    "blob_name = \"TestCase1\"\n",
    "mode = \"search\" \n",
    "language = \"en\"\n",
    "blob_Connection_String = \"dummy\"\n",
    "blob_container_name = \"dummy\"\n",
    "base_threshold = 1000\n",
    "buffer_percent = 10\n",
    "overlap_sent_count = 2\n",
    "\n",
    "\n",
    "import ast\n",
    "import json\n",
    "\n",
    "with open(\"page_map.txt\", \"r\") as file:\n",
    "    page_map_content = file.read()\n",
    "\n",
    "# If your file is like [(1, 0, 'text'), ...]\n",
    "#page_map = ast.literal_eval(page_map_content)\n",
    "page_map = [(1, 0, page_map_content)]\n",
    "\n",
    "# Then pass it to your function\n",
    "result_embedding = create_sections_code(\n",
    "    category_id, blob_name, page_map, mode, language,\n",
    "    blob_Connection_String, blob_container_name,\n",
    "    base_threshold, buffer_percent, overlap_sent_count\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63d0849e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************* result_embedding *************************************\n",
      "['page_map = [ (1, 0, \"\"\" INTRODUCTION ============ This document is designed to test the text splitting and chunking capabilities of your system. OVERVIEW -------- We include multiple paragraphs, code snippets, and lists to mimic real-world documents. Here is a Python function example: def factorial(n): if n == 0: return 1 else: return n * factorial(n-1) Use this to calculate factorial numbers. - First point - Second point with emphasis on bold text - Third point with an example link: https://example.com End of page one. \"\"\"), (2, 0, \"\"\" JAVASCRIPT SECTION ================== Now some JavaScript code for string manipulation: function reverseString(str) { return str.split(\\'\\').reverse().join(\\'\\'); } console.log(reverseString(\"Hello World!\")); DETAILS ------- The above function reverses any string you pass to it. This section also contains some bullet points: 1. Point one 2. Point two 3. Point three Testing sentence overlaps and paragraph boundaries is critical. End of page two', 'Point three Testing sentence overlaps and paragraph boundaries is critical. End of page two . Point three Testing sentence overlaps and paragraph boundaries is critical. End of page two. \"\"\"), (3, 0, \"\"\" FINAL NOTES =========== In conclusion, this document serves as a comprehensive example to evaluate chunking strategies. SUMMARY ------- We covered Python, JavaScript, headers, lists, and paragraphs. The idea is to simulate varied content. Additional points: * Chunk size management * Handling nested structures * Overlapping sentences between chunks Thank you for testing this with your system! End of document. \"\"\") ]']\n"
     ]
    }
   ],
   "source": [
    "print(f\"************************************* result_embedding *************************************\\n{result_embedding}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34555fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eda3be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
